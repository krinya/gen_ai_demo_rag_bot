"""
Main LangGraph-based Chatbot Implementation

This module implements a sophisticated chatbot using LangGraph StateGraph with:
- Conditional routing between FAQ, RAG, and LLM sources
- Answer quality grading with automatic rephrasing
- Conversation memory and session management
- LangSmith tracing for debugging and monitoring
"""

import os
import uuid
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Literal
from dotenv import load_dotenv
from pydantic import BaseModel, Field

# LangChain and LangGraph imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document, BaseMessage, HumanMessage, AIMessage
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.sqlite import SqliteSaver
import asyncio

# Local imports
from prompts.router_prompts import STRUCTURED_ROUTER_PROMPT, ANSWER_GRADING_PROMPT, QUESTION_REPHRASE_PROMPT

# Configure logging
logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING) # this turns off httpx logging
logger = logging.getLogger(__name__)

# Pydantic model for structured routing
class RouteDecision(BaseModel):
    """Structured output for routing decision"""
    route: Literal["faq", "rag", "llm"] = Field(
        description="The knowledge source to use for answering"
    )
    confidence: float = Field(
        ge=0.0, le=1.0,
        description="Confidence in the routing decision (0-1)"
    )
    reasoning: str = Field(
        description="Brief explanation for the routing choice"
    )

# Pydantic model for answer grading
class AnswerGrade(BaseModel):
    """Structured output for answer quality grading"""
    quality: Literal["good", "bad"] = Field(
        description="The quality assessment of the answer"
    )
    score: int = Field(
        ge=1, le=10,
        description="Numerical score from 1-10"
    )
    reasoning: str = Field(
        description="Explanation for the quality assessment"
    )
    confidence: float = Field(
        ge=0.0, le=1.0,
        description="Confidence in the grading decision (0-1)"
    )

# Pydantic model for chatbot configuration
class ChatbotConfig(BaseModel):
    """Configuration settings for the chatbot"""
    model_name: str = Field(default="gpt-4o-mini", description="OpenAI model to use")
    temperature: float = Field(default=0.3, ge=0.0, le=2.0, description="Model temperature")
    max_rephrase_attempts: int = Field(default=3, ge=1, le=10, description="Maximum rephrase attempts")
    embedding_model: str = Field(default="text-embedding-3-small", description="Embedding model for RAG")
    rag_documents_count: int = Field(default=5, ge=1, le=20, description="Number of documents to retrieve")
    langsmith_project: str = Field(default="template-chatbot", description="LangSmith project name")
    
    class Config:
        validate_assignment = True

# State definition for LangGraph
class ChatbotState(TypedDict):
    """State definition for the chatbot workflow"""
    messages: Annotated[List[BaseMessage], add_messages]  # Conversation messages
    question: str  # User's question or input
    context: str  # Conversation context for routing decisions
    route_decision: Literal["faq", "rag", "llm"]  # Decision made by the router
    current_answer: str  # The answer generated by the current source
    answer_quality: Literal["good", "bad"]  # Quality of the answer
    rephrase_attempts: int  # Number of rephrase attempts made
    session_id: str  # Unique session ID for conversation tracking
    original_question: str  # Original question for reference
    current_source: Literal["faq", "rag", "llm"]  # Track current source
    failed_sources: List[Literal["faq", "rag", "llm"]]  # Track which sources have failed

class TemplateChatbot:
    """
    Advanced chatbot template using LangGraph for workflow management you can use as a starting point.
    """
    
    def __init__(self, session_id: Optional[str] = None, config: Optional[ChatbotConfig] = None):
        """Initialize the chatbot with environment and components"""
        self.session_id = session_id or str(uuid.uuid4())
        self.config = config or ChatbotConfig()
        
        # Load environment and initialize components
        self._load_environment()
        self._initialize_llm()
        self._load_faq_content()
        self._initialize_rag_retriever()
        self._setup_langgraph()
        
        logger.info(f"Chatbot initialized with session ID: {self.session_id}")
        logger.info(f"Configuration: model={self.config.model_name}, temp={self.config.temperature}, max_attempts={self.config.max_rephrase_attempts}")
    
    def _load_environment(self):
        """Load environment variables and configure LangSmith tracing"""
        load_dotenv()
        
        # Validate required environment variables
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        
        # Configure LangSmith tracing if available
        langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
        if langsmith_api_key:
            os.environ["LANGCHAIN_TRACING_V2"] = "true"
            os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
            
            # Use project from .env if available, otherwise use config default
            langsmith_project = os.getenv("LANGSMITH_PROJECT", self.config.langsmith_project)
            os.environ["LANGCHAIN_PROJECT"] = langsmith_project
            
            # Set endpoint if provided
            langsmith_endpoint = os.getenv("LANGSMITH_ENDPOINT")
            if langsmith_endpoint:
                os.environ["LANGCHAIN_ENDPOINT"] = langsmith_endpoint
            
            logger.info(f"LangSmith tracing enabled for project: {langsmith_project}")
        else:
            logger.warning("LANGSMITH_API_KEY not found - tracing disabled")
    
    def _initialize_llm(self):
        """Initialize the OpenAI language model"""
        self.llm = ChatOpenAI(
            model=self.config.model_name,
            temperature=self.config.temperature,
            api_key=self.openai_api_key
        )
        logger.info(f"OpenAI LLM initialized: {self.config.model_name}")
    
    def _load_faq_content(self):
        """Load FAQ content from markdown file"""
        faq_path = Path(__file__).parent / "faq" / "faq.md"
        
        try:
            with open(faq_path, 'r', encoding='utf-8') as f:
                self.faq_content = f.read()
            logger.info(f"FAQ content loaded: {len(self.faq_content)} characters")
        except FileNotFoundError:
            logger.warning(f"FAQ file not found at {faq_path}")
            self.faq_content = "No FAQ content available."
    
    def _initialize_rag_retriever(self):
        """Initialize RAG retriever with ChromaDB"""
        rag_storage_path = Path(__file__).parent / "rag_storage"
        
        if rag_storage_path.exists():
            try:
                embeddings = OpenAIEmbeddings(
                    model=self.config.embedding_model,
                    api_key=self.openai_api_key
                )
                
                self.vector_store = Chroma(
                    persist_directory=str(rag_storage_path),
                    embedding_function=embeddings,
                    collection_name="rag_documents",
                    collection_metadata={"hnsw:space": "cosine"}  # Use cosine similarity for semantic search
                )
                
                self.rag_retriever = self.vector_store.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": self.config.rag_documents_count}
                )
                logger.info(f"RAG retriever initialized successfully with {self.config.rag_documents_count} documents")
                
            except Exception as e:
                logger.error(f"Error initializing RAG retriever: {e}")
                self.rag_retriever = None
        else:
            logger.warning(f"RAG storage not found at {rag_storage_path}")
            self.rag_retriever = None
    
    def _setup_langgraph(self):
        """Set up the LangGraph workflow"""
        # Create the state graph
        workflow = StateGraph(ChatbotState)
        
        # Add nodes
        workflow.add_node("route_question", self._route_question)
        workflow.add_node("answer_faq", self._answer_faq)
        workflow.add_node("answer_rag", self._answer_rag)
        workflow.add_node("answer_llm", self._answer_llm)
        workflow.add_node("grade_answer", self._grade_answer)
        workflow.add_node("rephrase_question", self._rephrase_question)
        workflow.add_node("final_response", self._final_response)
        
        # Add edges
        workflow.add_edge(START, "route_question")
        
        # Conditional routing based on decision
        workflow.add_conditional_edges(
            "route_question",
            self._route_decision,
            {
                "faq": "answer_faq",
                "rag": "answer_rag", 
                "llm": "answer_llm"
            }
        )
        
        # All answer nodes go to grading
        workflow.add_edge("answer_faq", "grade_answer")
        workflow.add_edge("answer_rag", "grade_answer")
        workflow.add_edge("answer_llm", "grade_answer")
        
        # Conditional edge from grading with alternative source logic
        workflow.add_conditional_edges(
            "grade_answer",
            self._grade_decision,
            {
                "good": "final_response",
                "bad": "rephrase_question",
                "max_attempts": "final_response",  # After max attempts, return what we have
                "try_alternative": "route_question"  # Try alternative source
            }
        )
        
        workflow.add_edge("rephrase_question", "route_question")
        workflow.add_edge("final_response", END)
        
        # Set up memory checkpointing for conversation memory
        from langgraph.checkpoint.memory import MemorySaver
        
        # Use in-memory checkpointer (can be upgraded to persistent later)
        # This provides conversation memory within the session
        checkpointer = MemorySaver()
        
        # Compile with checkpointer for memory
        self.app = workflow.compile(checkpointer=checkpointer)
        
        logger.info("LangGraph workflow compiled successfully with memory checkpointing")
    
    # Node implementations
    async def _route_question(self, state: ChatbotState) -> ChatbotState:
        """Route the question to appropriate knowledge source using structured Pydantic output"""
        # Use structured output for robust routing
        structured_llm = self.llm.with_structured_output(RouteDecision)
        prompt = ChatPromptTemplate.from_template(STRUCTURED_ROUTER_PROMPT)
        chain = prompt | structured_llm
        
        # Build conversation context from messages
        context = self._build_conversation_context(state["messages"])
        
        # Check if we should try alternative source due to previous failures
        failed_sources = state.get("failed_sources", [])
        
        # Get structured routing decision
        decision = await chain.ainvoke({
            "question": state["question"],
            "context": context
        })
        
        route_decision = decision.route
        confidence = decision.confidence
        reasoning = decision.reasoning
        
        logger.info(f"Structured route decision: {route_decision} (confidence: {confidence:.2f})")
        logger.info(f"Reasoning: {reasoning}")
        
        # If this source already failed 3 times, try alternative
        if failed_sources.count(route_decision) >= 3:
            alternative_sources = [s for s in ["faq", "rag", "llm"] if s != route_decision]
            if alternative_sources:
                # Try the first alternative that hasn't failed as much
                for alt in alternative_sources:
                    if failed_sources.count(alt) < 3:
                        logger.info(f"Switching to alternative source: {alt} (original {route_decision} failed {failed_sources.count(route_decision)} times)")
                        route_decision = alt
                        break
        
        return {
            **state, 
            "route_decision": route_decision,
            "current_source": route_decision,
            "context": context
        }
    
    def _build_conversation_context(self, messages: List[BaseMessage]) -> str:
        """Build conversation context from message history"""
        if not messages:
            return ""
        
        # Get last 5 messages for context (to avoid token limits)
        recent_messages = messages[-5:]
        context_parts = []
        
        for msg in recent_messages:
            if isinstance(msg, HumanMessage):
                context_parts.append(f"User: {msg.content}")
            elif isinstance(msg, AIMessage):
                context_parts.append(f"Assistant: {msg.content}")
        
        return "\n".join(context_parts)
    
    async def _answer_faq(self, state: ChatbotState) -> ChatbotState:
        """Answer using FAQ content"""
        faq_prompt = ChatPromptTemplate.from_template("""
        You are a helpful assistant answering questions using the provided FAQ content.
        
        FAQ Content:
        {faq_content}
        
        Question: {question}
        Context: {context}
        
        Please provide a helpful answer based on the FAQ. If the FAQ doesn't contain 
        relevant information, say so clearly.
        
        Answer:
        """)
        
        chain = faq_prompt | self.llm | StrOutputParser()
        
        answer = await chain.ainvoke({
            "faq_content": self.faq_content,
            "question": state["question"],
            "context": state.get("context", "")
        })
        
        logger.info("Generated FAQ-based answer")
        return {**state, "current_answer": answer}
    
    async def _answer_rag(self, state: ChatbotState) -> ChatbotState:
        """Answer using RAG retrieval"""
        if not self.rag_retriever:
            # Fallback to LLM if RAG not available
            return await self._answer_llm(state)
        
        try:
            # Retrieve relevant documents
            docs = await self.rag_retriever.ainvoke(state["question"])
            
            if not docs:
                # No relevant documents found, fallback to LLM
                return await self._answer_llm(state)
            
            # Format context from retrieved documents
            context_text = "\n\n".join([
                f"Source: {doc.metadata.get('source', 'Unknown')}\n{doc.page_content}"
                for doc in docs
            ])
            
            rag_prompt = ChatPromptTemplate.from_template("""
            You are a helpful assistant answering questions using the provided context documents.
            
            Context Documents:
            {context_docs}
            
            Question: {question}
            Conversation Context: {context}
            
            Please provide a comprehensive answer based on the context documents. 
            If the documents don't contain enough information, mention what's missing if you can.
            
            Answer:
            """)
            
            chain = rag_prompt | self.llm | StrOutputParser()
            
            answer = await chain.ainvoke({
                "context_docs": context_text,
                "question": state["question"],
                "context": state.get("context", "")
            })
            
            logger.info(f"Generated RAG-based answer using {len(docs)} documents")
            return {**state, "current_answer": answer}
            
        except Exception as e:
            logger.error(f"Error in RAG retrieval: {e}")
            return await self._answer_llm(state)
    
    async def _answer_llm(self, state: ChatbotState) -> ChatbotState:
        """Answer using general LLM knowledge"""
        llm_prompt = ChatPromptTemplate.from_template("""
        You are a helpful AI assistant. Answer the user's question using your general knowledge.
        
        Question: {question}
        Context: {context}
        
        Provide a helpful and accurate response. If you're not sure about something, 
        please say so rather than guessing.
        
        Answer:
        """)
        
        chain = llm_prompt | self.llm | StrOutputParser()
        
        answer = await chain.ainvoke({
            "question": state["question"],
            "context": state.get("context", "")
        })
        
        logger.info("Generated LLM-based answer")
        return {**state, "current_answer": answer}
    
    async def _grade_answer(self, state: ChatbotState) -> ChatbotState:
        """Grade the quality of the current answer using structured output"""
        # Use structured output for robust grading
        structured_llm = self.llm.with_structured_output(AnswerGrade)
        grade_prompt = ChatPromptTemplate.from_template(ANSWER_GRADING_PROMPT)
        chain = grade_prompt | structured_llm
        
        grade_result = await chain.ainvoke({
            "question": state["question"],
            "answer": state["current_answer"],
            "source": state["route_decision"]
        })
        
        quality = grade_result.quality
        score = grade_result.score
        reasoning = grade_result.reasoning
        confidence = grade_result.confidence
        
        logger.info(f"Answer graded as: {quality} (score: {score}/10, confidence: {confidence:.2f})")
        logger.info(f"Grading reasoning: {reasoning}")
        
        return {**state, "answer_quality": quality}
    
    async def _rephrase_question(self, state: ChatbotState) -> ChatbotState:
        """Rephrase the question to try to get a better answe, try to rephrase the question to improve answer quality."""
        attempts = state.get("rephrase_attempts", 0) + 1
        failed_sources = state.get("failed_sources", [])
        current_source = state.get("current_source", "")
        
        # Track failed source
        if current_source and current_source not in failed_sources:
            failed_sources = failed_sources + [current_source]
        
        if attempts > self.config.max_rephrase_attempts:
            logger.info("Maximum rephrase attempts reached")
            return {
                **state, 
                "rephrase_attempts": attempts,
                "failed_sources": failed_sources
            }
        
        rephrase_prompt = ChatPromptTemplate.from_template(QUESTION_REPHRASE_PROMPT)
        chain = rephrase_prompt | self.llm | StrOutputParser()
        
        rephrased = await chain.ainvoke({
            "original_question": state.get("original_question", state["question"]),
            "poor_answer": state["current_answer"],
            "context": state.get("context", ""),
            "attempt_number": attempts
        })
        
        # Store original question if this is the first rephrase
        original_q = state.get("original_question", state["question"])
        
        logger.info(f"Rephrased question (attempt {attempts}): {rephrased}")
        
        return {
            **state, 
            "question": rephrased,
            "original_question": original_q,
            "rephrase_attempts": attempts,
            "failed_sources": failed_sources
        }
    
    async def _final_response(self, state: ChatbotState) -> ChatbotState:
        """Prepare the final response"""
        if state.get("rephrase_attempts", 0) >= self.config.max_rephrase_attempts and state["answer_quality"] == "bad":
            final_answer = (
                "I apologize, but I'm having difficulty providing a satisfactory answer to your question. "
                "Could you please rephrase your question or provide more context? You can also try asking "
                "a more specific question, and I'll do my best to help you."
            )
        else:
            final_answer = state["current_answer"]
        
        # Add the AI response to the conversation history
        ai_message = AIMessage(content=final_answer)
        
        return {
            **state,
            "current_answer": final_answer,
            "messages": state["messages"] + [ai_message]
        }
    
    # Decision functions for conditional edges
    def _route_decision(self, state: ChatbotState) -> str:
        """Determine routing based on route_decision"""
        return state["route_decision"]
    
    def _grade_decision(self, state: ChatbotState) -> str:
        """Determine next step based on answer quality"""
        if state["answer_quality"] == "good":
            return "good"
        
        current_source = state.get("current_source", "")
        rephrase_attempts = state.get("rephrase_attempts", 0)
        failed_sources = state.get("failed_sources", [])
        
        # Special case: For LLM answers, don't rephrase - just accept the answer
        if current_source == "llm":
            logger.info("LLM answer - accepting without rephrasing")
            return "good"
        
        # Check if we've reached max attempts for this source
        if rephrase_attempts >= self.config.max_rephrase_attempts:
            # Try alternative source if current source failed 3 times
            source_failures = failed_sources.count(current_source)
            if source_failures >= 3:
                # Try alternative source
                alternative_sources = [s for s in ["faq", "rag"] if s != current_source]
                if alternative_sources and any(failed_sources.count(alt) < 3 for alt in alternative_sources):
                    logger.info(f"Max attempts reached for {current_source}, trying alternative source")
                    return "try_alternative"
            
            return "max_attempts"
        else:
            return "bad"
    
    # Public methods
    async def chat(self, user_input: str, context: str = "") -> str:
        """
        Main chat method - processes user input and returns response
        
        Args:
            user_input: The user's question or message
            context: Additional context for the conversation
            
        Returns:
            The chatbot's response
        """
        try:
            # Create initial state - LangGraph will handle message persistence automatically
            initial_state = ChatbotState(
                messages=[HumanMessage(content=user_input)],
                question=user_input,
                context=context,
                route_decision="",
                current_answer="",
                answer_quality="",
                rephrase_attempts=0,
                session_id=self.session_id,
                original_question=user_input,
                current_source="",
                failed_sources=[]
            )
            
            # Run the workflow with thread configuration for memory
            config = {"configurable": {"thread_id": self.session_id}, "run_name": "chat log"}
            result = await self.app.ainvoke(initial_state, config)
            
            return result["current_answer"]
            
        except Exception as e:
            logger.error(f"Error in chat method: {e}")
            return "I apologize, but I encountered an error while processing your request. Please try again."
    
    def get_conversation_history(self) -> List[BaseMessage]:
        """Get the conversation history for the current session"""
        try:
            config = {"configurable": {"thread_id": self.session_id}, "run_name": "chat log"}
            # Get the latest state
            state = self.app.get_state(config)
            if state.values:
                return state.values.get("messages", [])
            return []
        except Exception as e:
            logger.error(f"Error retrieving conversation history: {e}")
            return []
    
    def reset_conversation(self):
        """Reset the conversation for the current session"""
        try:
            # Create new session ID to start fresh
            self.session_id = str(uuid.uuid4())
            logger.info(f"Conversation reset with new session ID: {self.session_id}")
        except Exception as e:
            logger.error(f"Error resetting conversation: {e}")
    
    def new_session(self) -> str:
        """Create a new conversation session and return the new session ID"""
        self.session_id = str(uuid.uuid4())
        logger.info(f"New conversation session created: {self.session_id}")
        return self.session_id

# Convenience function for external usage
async def create_chatbot(session_id: Optional[str] = None, config: Optional[ChatbotConfig] = None) -> TemplateChatbot:
    """Create and return a configured chatbot instance"""
    return TemplateChatbot(session_id=session_id, config=config)

if __name__ == "__main__":
    
    async def test_chatbot():
        """Simple test of the chatbot functionality"""
        chatbot = await create_chatbot()
        
        # Test questions
        test_questions = [
            "How much does your service cost?",
            "What is CleanGo?",
            "How does machine learning work?"
        ]
        
        for question in test_questions:
            print(f"\nðŸ¤– Question: {question}")
            response = await chatbot.chat(question)
            print(f"ðŸ’¬ Response: {response}")
            print("-" * 50)
    
    # Run test
    asyncio.run(test_chatbot())